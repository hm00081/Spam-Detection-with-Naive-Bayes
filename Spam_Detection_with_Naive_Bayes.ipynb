{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-fuPyC9FsJb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "#spam.csv íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ì„ ì‚­ì œ\n",
        "#ë ˆì´ë¸”ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ (ham -> 0, spam -> 1)\n",
        "#ì¤‘ë³µëœ í…ìŠ¤íŠ¸ë¥¼ ì‚­ì œ\n",
        "\n",
        "#2. í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \n",
        "#train_test_splitì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• \n",
        "\n",
        "#3. í† í°í™”\n",
        "#Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ë¥¼ í† í°í™”\n",
        "#ì´ í† í°í™”ëœ ë°ì´í„°ëŠ” RNN ëª¨ë¸ì— ì‚¬ìš©\n",
        "\n",
        "#4. ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”\n",
        "#ìŠ¤íŒ¸ ë©”ì¼ê³¼ ì •ìƒ ë©”ì¼ì˜ ë¶„í¬ë¥¼ í™•ì¸\n",
        "#ë©”ì¼ì˜ ê¸¸ì´ ë¶„í¬ì— ëŒ€í•œ ì‹œê°í™”ë„ ì§„í–‰\n",
        "\n",
        "#5. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "#NLTKì˜ stopwordsì™€ êµ¬ë‘ì (punctuation)ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì¼ ë‚´ì˜ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë‚˜ ë¬¸ìžë¥¼ ì œê±°\n",
        "#ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì—ì„œ ìŠ¤íŒ¸ ë©”ì¼ê³¼ ì •ìƒ ë©”ì¼ì˜ ê°€ìž¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë¥¼ í™•ì¸\n",
        "\n",
        "#6. RNNì„ ì‚¬ìš©í•œ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜\n",
        "#ìž„ë² ë”©ì¸µ, RNN, ë° Dense ì¸µì„ í¬í•¨í•˜ëŠ” ì‹¬ì¸µ ì‹ ê²½ë§ì„ êµ¬ì„±\n",
        "#ëª¨ë¸ì„ í›ˆë ¨ ë°ì´í„°ì— í•™ìŠµì‹œí‚¤ê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€\n",
        "\n",
        "#7. ëª¨ë¸ì˜ ì†ì‹¤ ì‹œê°í™”\n",
        "#í›ˆë ¨ ë° ê²€ì¦ ì†ì‹¤ì„ ì‹œê°í™”í•˜ì—¬ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì„ í™•ì¸\n"
      ],
      "metadata": {
        "id": "Sz0Uu8s0lvcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spam mail data url\n",
        "url = \"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv\"\n",
        "\n",
        "urllib.request.urlretrieve(url, filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "print('ì´ ë©”ì¼ ìˆ˜ :',len(data))"
      ],
      "metadata": {
        "id": "bVCvPoF_Fxy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Raw data head\n",
        "data.head()"
      ],
      "metadata": {
        "id": "jzvkgm5xmTTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unwanted columns and rename remaining columns\n",
        "if len(data.columns) > 3:\n",
        "    data = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\n",
        "if data.columns[0] != 'label':\n",
        "    data = data.rename(columns={'v1': 'label', 'v2': 'text'}) # ham :ì¼ë°˜, spam: ìŠ¤íŒ¸\n",
        "data.head()"
      ],
      "metadata": {
        "id": "i4Gx-gUyLVef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "h213IOQF1sCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('label').describe()"
      ],
      "metadata": {
        "id": "DJCaQEjE1xQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert label to numerical variable (ì •ìˆ˜í˜•íƒœë¡œ êµ¬ë¶„)\n",
        "data['label'] = data.label.map({'ham': 0, 'spam': 1}) # spam to 1, ham to 0\n",
        "data['message_len'] = data['text'].apply(len)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "5eA5WhzHoHz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete duplicate E-mail\n",
        "data.drop_duplicates(subset=['text'], inplace=True)\n",
        "print('ì¤‘ë³µì‚­ì œí•œ ì´ë©”ì¼ ìˆ˜ :',len(data))"
      ],
      "metadata": {
        "id": "BDIhAsw1qmHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = data['text']\n",
        "y_data = data['label'] #data1,2 to x, y data"
      ],
      "metadata": {
        "id": "VV3nW7sQpKr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, stratify=y_data) # 0.2ë¡œ í–ˆëŠ”ë°, 0.1ë¡œ í•˜ë©´?"
      ],
      "metadata": {
        "id": "gOlGqsfcpNdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "print(X_train_encoded[:5])"
      ],
      "metadata": {
        "id": "dIR9PBnprWK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Words key-value dictionary\n",
        "word_to_index = tokenizer.word_index\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "id": "2Fy2QN6QwT3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•© ì‚¬ì´ì¦ˆ: {}'.format((vocab_size)))"
      ],
      "metadata": {
        "id": "5YINrsI2wcIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 189\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "print(\"í›ˆë ¨ ë°ì´í„°ì˜ í¬ê¸°:\", X_train_padded.shape)"
      ],
      "metadata": {
        "id": "-Nux_3oXw5Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# nltk data download(stopwords)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def text_process(mess):\n",
        "    \"\"\"\n",
        "    Takes in a string of text, then performs the following:\n",
        "    1. Remove all punctuation\n",
        "    2. Remove all stopwords\n",
        "    3. Returns a list of the cleaned text\n",
        "    \"\"\"\n",
        "    STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure', 'ltgt']\n",
        "    # Check characters to see if they are in punctuation\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\n",
        "\n",
        "    # Join the characters again to form the string.\n",
        "    nopunc = ''.join(nopunc)\n",
        "\n",
        "    # Now just remove any stopwords\n",
        "    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n",
        "\n",
        "# 'text' ì»¬ëŸ¼ì— text_process í•¨ìˆ˜ ì ìš©\n",
        "data['clean_msg'] = data['text'].apply(text_process)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "jP_0o0ec48iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë‘ ê°œì˜ ë¹ˆ ë”•ì…”ë„ˆë¦¬(ìŠ¤íŒ¸, ì¼ë°˜) ì´ˆê¸°í™”í•˜ì—¬ ìŠ¤íŒ¸ ë©”ì‹œì§€ì™€ ì¼ë°˜ ë©”ì‹œì§€ ë‹¨ì–´ ë¹ˆë„ ì €ìž¥\n",
        "ham_words = {}\n",
        "spam_words = {}\n",
        "\n",
        "for idx, email in data.iterrows():\n",
        "    for word in email['clean_msg'].split():\n",
        "        if email['label'] == 0:\n",
        "            # í•´ë‹¹ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ham_words ë”•ì…”ë„ˆë¦¬ì—ì„œ 1 ì¦ê°€\n",
        "            # ë§Œì•½ í•´ë‹¹ ë‹¨ì–´ê°€ ë”•ì…”ë„ˆë¦¬ì— ì—†ë‹¤ë©´ ê¸°ë³¸ê°’ 0ì„ ì‚¬ìš©\n",
        "            ham_words[word] = ham_words.get(word, 0) + 1\n",
        "        else:\n",
        "            # ë§Œì•½ í•´ë‹¹ ì´ë©”ì¼ì´ ìŠ¤íŒ¸ ë©”ì‹œì§€ë¼ë©´\n",
        "            # í•´ë‹¹ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ spam_words ë”•ì…”ë„ˆë¦¬ì—ì„œ 1 ì¦ê°€\n",
        "            # ë§Œì•½ í•´ë‹¹ ë‹¨ì–´ê°€ ë”•ì…”ë„ˆë¦¬ì— ì—†ë‹¤ë©´ ê¸°ë³¸ê°’ 0ì„ ì‚¬ìš©\n",
        "            spam_words[word] = spam_words.get(word, 0) + 1\n",
        "\n",
        "print(ham_words)\n",
        "print(spam_words)"
      ],
      "metadata": {
        "id": "WvlxOeP-GQcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_ham_words = sum(ham_words.values())  # í–„ ë©”ì‹œì§€ì—ì„œì˜ ì´ ë‹¨ì–´ ìˆ˜\n",
        "total_spam_words = sum(spam_words.values())  # ìŠ¤íŒ¸ ë©”ì‹œì§€ì—ì„œì˜ ì´ ë‹¨ì–´ ìˆ˜\n",
        "\n",
        "print(total_ham_words, total_spam_words)\n",
        "\n",
        "# ham_wordsì™€ spam_words ë”•ì…”ë„ˆë¦¬ì˜ í‚¤(ë‹¨ì–´)ë“¤ì„ í•©ì³ì„œ ì¤‘ë³µì„ ì œê±°\n",
        "total_words = len(set(list(ham_words.keys()) + list(spam_words.keys())))  # ì „ì²´ ê³ ìœ  ë‹¨ì–´ ìˆ˜\n",
        "\n",
        "print(total_words)\n",
        "\n",
        "N_ham = len(data[data['label'] == 0])  # í–„ ë©”ì‹œì§€ì˜ ìˆ˜\n",
        "N_spam = len(data[data['label'] == 1])  # ìŠ¤íŒ¸ ë©”ì‹œì§€ì˜ ìˆ˜\n",
        "N_total = len(data)  # ì „ì²´ ë©”ì‹œì§€ì˜ ìˆ˜\n",
        "\n",
        "print(N_ham, N_spam, N_total)\n",
        "\n",
        "# ð‘ƒ Ì‚(ð‘_ð‘— )=ð‘_(ð‘_ð‘— )/ð‘_ð‘¡ð‘œð‘¡ð‘Žð‘™ ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ í–„ ë° ìŠ¤íŒ¸ ë©”ì‹œì§€ì˜ ë¹„ìœ¨ì„ ê³„ì‚°\n",
        "P_ham = N_ham / N_total  # í–„ ë©”ì‹œì§€ì˜ ë¹„ìœ¨\n",
        "P_spam = N_spam / N_total  # ìŠ¤íŒ¸ ë©”ì‹œì§€ì˜ ë¹„ìœ¨\n",
        "\n",
        "print(P_ham, P_spam) # ì¼ë°˜, ìŠ¤íŒ¸ ë¹„ìœ¨"
      ],
      "metadata": {
        "id": "tb9IR9wAGUCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# score ê³„ì‚°\n",
        "def calculate_score(sentence, label_words, total_words_class):\n",
        "    score = 1\n",
        "    for word in sentence.split():\n",
        "        if word in label_words:\n",
        "            score *= (label_words[word] + 1) / (total_words_class + total_words)\n",
        "        else:\n",
        "            score *= 1 / (total_words_class + total_words)\n",
        "    return score"
      ],
      "metadata": {
        "id": "dK4q2gBTsbXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ð‘(ð‘¤_ð‘–â”‚ð‘)= (ð‘ð‘œð‘¢ð‘›ð‘¡(ð‘¤_ð‘–,ð‘)+1)/((âˆ‘_(ð‘¤âˆˆð‘‰) ð‘ð‘œð‘¢ð‘›ð‘¡(ð‘¤,ð‘) )+|ð‘‰|)\n",
        "def calculate_ham_spam_scores(email):\n",
        "    ham_score = P_ham * calculate_score(email, ham_words, total_ham_words)\n",
        "    spam_score = P_spam * calculate_score(email, spam_words, total_spam_words)\n",
        "    return ham_score, spam_score\n",
        "\n",
        "# ê° ë©”ì¼ì— ëŒ€í•œ ìŠ¤íŒ¸ ë° í–„ ì ìˆ˜ë¥¼ ê³„ì‚°\n",
        "scores = X_test.apply(calculate_ham_spam_scores)\n",
        "\n",
        "#print(scores)\n",
        "\n",
        "# ê³„ì‚°ëœ ì ìˆ˜ë¥¼ DataFrameì— ìƒˆë¡œìš´ ì—´ë¡œ ì¶”ê°€\n",
        "X_test_with_scores = X_test.to_frame()  # Series to DataFrame\n",
        "X_test_with_scores['ham_score'] = scores.apply(lambda x: x[0])\n",
        "X_test_with_scores['spam_score'] = scores.apply(lambda x: x[1])\n",
        "\n",
        "print(X_test_with_scores.head())\n"
      ],
      "metadata": {
        "id": "Do8lwf2oGaGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# 1. í•™ìŠµ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ spam/ham ë‹¨ì–´ ë¹ˆë„ë¥¼ ê³„ì‚°\n",
        "train_data = pd.DataFrame({'text': X_train, 'label': y_train})\n",
        "\n",
        "words = train_data[train_data.label==0].text.apply(lambda x: [word.lower() for word in x.split()])\n",
        "ham_words = Counter()\n",
        "for msg in words:\n",
        "    ham_words.update(msg)\n",
        "\n",
        "words = train_data[train_data.label==1].text.apply(lambda x: [word.lower() for word in x.split()])\n",
        "spam_words = Counter()\n",
        "for msg in words:\n",
        "    spam_words.update(msg)\n",
        "\n",
        "# 2. ì´ ë‹¨ì–´ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°\n",
        "scores = X_test.apply(calculate_ham_spam_scores)\n",
        "X_test_with_scores = X_test.to_frame()\n",
        "X_test_with_scores['ham_score'] = scores.apply(lambda x: x[0])\n",
        "X_test_with_scores['spam_score'] = scores.apply(lambda x: x[1])\n",
        "\n",
        "# ì›ë³¸ ë°ì´í„°ì— ìŠ¤ì½”ì–´ ì¶”ê°€\n",
        "data = pd.merge(data, X_test_with_scores[['ham_score', 'spam_score']], left_index=True, right_index=True, how='left')\n",
        "\n",
        "# ... ì¤‘ëžµ ...\n",
        "#ì´ë ‡ê²Œ ìœ„ì¹˜ì‹œí‚¤ë©´ í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤íŒ¸/í–„ ë‹¨ì–´ ë¹ˆë„ë¥¼ ê³„ì‚°í•˜ê³ , ê·¸ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ë” ìžì—°ìŠ¤ëŸ¬ì›Œì§‘ë‹ˆë‹¤.\n",
        "\n",
        "# ì›ë³¸ ë°ì´í„°ì— ìŠ¤ì½”ì–´ ì¶”ê°€\n",
        "#data.drop(columns=['ham_score_x', 'spam_score_x', 'ham_score_y', 'spam_score_y'], inplace=True)\n",
        "\n",
        "#data = pd.merge(data, X_test_with_scores[['ham_score', 'spam_score']], left_index=True, right_index=True, how='left')\n",
        "data = pd.merge(data, X_test_with_scores[['ham_score', 'spam_score']], left_index=True, right_index=True, how='left', suffixes=('', '_test'))\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "9tp9YuD5Ga4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_probabilities(sentence, label_words, total_words_class):\n",
        "    word_probabilities = {}\n",
        "    for word in sentence.split():\n",
        "        if word in label_words:\n",
        "            word_probabilities[word] = (label_words[word] + 1) / (total_words_class + total_words)\n",
        "        else:\n",
        "            word_probabilities[word] = 1 / (total_words_class + total_words)\n",
        "    return word_probabilities\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ head ë¶€ë¶„ì— ëŒ€í•´ì„œ ê° ë‹¨ì–´ì˜ í™•ë¥ ì„ ê³„ì‚°\n",
        "word_probs_ham = X_test.head().apply(lambda x: calculate_word_probabilities(x, ham_words, total_ham_words))\n",
        "word_probs_spam = X_test.head().apply(lambda x: calculate_word_probabilities(x, spam_words, total_spam_words))\n",
        "\n",
        "print(\"Word Probabilities for Ham(ì¼ë°˜):\")\n",
        "print(word_probs_ham)\n",
        "print(\"\\nWord Probabilities for Spam(ìŠ¤íŒ¸):\")\n",
        "print(word_probs_spam)\n"
      ],
      "metadata": {
        "id": "IgFIYgClwr3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
        "y_pred_test = X_test_with_scores.apply(lambda row: 1 if row['spam_score'] > row['ham_score'] else 0, axis=1)\n",
        "#ìœ„ì˜ ì½”ë“œëŠ” X_test_with_scores DataFrameì˜ ê° í–‰ì— ëŒ€í•˜ì—¬ spam_scoreì™€ ham_scoreë¥¼ ë¹„êµ\n",
        "#ë§Œì•½ spam_scoreê°€ ham_scoreë³´ë‹¤ í¬ë‹¤ë©´, í•´ë‹¹ ë©”ì¼ì€ ìŠ¤íŒ¸(1)ìœ¼ë¡œ ì˜ˆì¸¡ë˜ê³  ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ í–„(0)ìœ¼ë¡œ ì˜ˆì¸¡ë¨\n",
        "\n",
        "# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í‰ê°€\n",
        "# ë§¤íŠ¸ë¦­ìŠ¤ ì¶œë ¥\n",
        "cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "print(\"Confusion Matrix for Test Data:\")\n",
        "print(cm_test)\n",
        "\n",
        "# ì •í™•ë„ ì¶œë ¥\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "print(f\"Accuracy for Test Data: {accuracy_test:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a5KX9IBKHIsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "True Negative (TN): ì‹¤ì œë¡œ í–„ ë©”ì¼ì¸ë°, í–„ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ.\n",
        "\n",
        "False Positive (FP): ì‹¤ì œë¡œ í–„ ë©”ì¼ì¸ë°, ìŠ¤íŒ¸ìœ¼ë¡œ ìž˜ëª» ì˜ˆì¸¡í•œ ê²ƒ.\n",
        "\n",
        "False Negative (FN): ì‹¤ì œë¡œ ìŠ¤íŒ¸ì¸ë°, í–„ìœ¼ë¡œ ìž˜ëª» ì˜ˆì¸¡í•œ ê²ƒ.\n",
        "\n",
        "True Positive (TP): ì‹¤ì œë¡œ ìŠ¤íŒ¸ì´ë©°, ìŠ¤íŒ¸ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ê²ƒ.\n",
        "\n",
        "TN = 850: 850ê°œì˜ í–„ ë©”ì¼ì´ ì˜¬ë°”ë¥´ê²Œ í–„ìœ¼ë¡œ ì˜ˆì¸¡\n",
        "\n",
        "FP = 53: 53ê°œì˜ í–„ ë©”ì¼ì´ ìŠ¤íŒ¸ìœ¼ë¡œ ìž˜ëª» ì˜ˆì¸¡\n",
        "\n",
        "FN = 12: 12ê°œì˜ ìŠ¤íŒ¸ ë©”ì¼ì´ í–„ìœ¼ë¡œ ìž˜ëª» ì˜ˆì¸¡\n",
        "\n",
        "TP = 119: 119ê°œì˜ ìŠ¤íŒ¸ ë©”ì¼ì´ ì˜¬ë°”ë¥´ê²Œ ìŠ¤íŒ¸ìœ¼ë¡œ ì˜ˆì¸¡\n",
        "\n",
        "Accuracy=  TP+TNâ€‹ / TP+TN+FP+FN\n",
        "\n",
        "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ëŠ” ì•½ 93.71%"
      ],
      "metadata": {
        "id": "qN1WfG8aMQIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- ì•„ëž˜ëŠ” ì´ì „ê³¼ì œ ì½”ë“œ ----"
      ],
      "metadata": {
        "id": "6i6_lT08fdvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "#ham(not spam) word count\n",
        "words = data[data.label==0].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\n",
        "ham_words = Counter()\n",
        "\n",
        "for msg in words:\n",
        "    ham_words.update(msg)\n",
        "\n",
        "print(ham_words.most_common(50))"
      ],
      "metadata": {
        "id": "iMLKBitg7d-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spam word count\n",
        "words = data[data.label==1].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\n",
        "spam_words = Counter()\n",
        "\n",
        "for msg in words:\n",
        "    spam_words.update(msg)\n",
        "\n",
        "print(spam_words.most_common(50))"
      ],
      "metadata": {
        "id": "eE2cDVi88vQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ham  = data[data['label'] == 0].copy()\n",
        "data_spam = data[data['label'] == 1].copy()"
      ],
      "metadata": {
        "id": "7ZLmoF9Q_Bhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_wordcloud(data_spam_or_ham, title):\n",
        "    text = ' '.join(data_spam_or_ham['text'].astype(str).tolist())\n",
        "    stopwords = set(wordcloud.STOPWORDS)\n",
        "\n",
        "    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',\n",
        "                    colormap='viridis', width=800, height=600).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10,7), frameon=True)\n",
        "    plt.imshow(fig_wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=20 )\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Z5VMX0Ac_GA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "\n",
        "print(len(X_train_encoded))\n",
        "print(len(X_train_padded))"
      ],
      "metadata": {
        "id": "igkUzzxvBiR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "from tensorflow.python.keras.engine.sequential import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "\n",
        "embedding_dim = 32\n",
        "hidden_units = 32\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train_padded, y_train.astype(int), epochs=4, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "TxJ6oFuMxCOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess for test data\n",
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen=max_len)\n",
        "\n",
        "#Model evaluation (ë”¥ëŸ¬ë‹ ê¸°ë°˜ ëª¨ë¸ì„±ëŠ¥)\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test.astype(int), batch_size=64)\n",
        "print(\"í…ŒìŠ¤íŠ¸ ì •í™•ë„:\", accuracy)"
      ],
      "metadata": {
        "id": "11TQbyKc_lMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "\n",
        "\n",
        "pipe = Pipeline([('bow', CountVectorizer()),\n",
        "                 ('tfid', TfidfTransformer()),\n",
        "                 ('model', MultinomialNB())]) # ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ"
      ],
      "metadata": {
        "id": "0Cm3yVgmJMIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RuXcTe1_JN-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "yru80uEfJRuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, f1_score\n",
        "\n",
        "# For Naive Bayes model\n",
        "print(\"=======Accuracy Score===========\")\n",
        "print(metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print Recall\n",
        "print(\"=======Recall Score===========\")\n",
        "print(recall_score(y_test, y_pred))\n",
        "\n",
        "# Print F1-Score\n",
        "print(\"=======F1 Score===========\")\n",
        "print(f1_score(y_test, y_pred))\n",
        "\n",
        "# print the confusion matrix\n",
        "print(\"=======Confusion Matrix===========\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qS_MksdFJVhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy (ì •í™•ë„) - 0.9526 (95.26%):\n",
        "\n",
        "95.26%ê°€ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡ë¨.\n",
        "(ë”¥ëŸ¬ë‹ ê¸°ë°˜ ëª¨ë¸ì€ 0.9806576371192932ë¡œ ë‚˜ì˜´)\n",
        "\n",
        "Recall (ìž¬í˜„ìœ¨) - 0.6259 (62.59%):\n",
        "\n",
        "ì‹¤ì œ SPAM ë©”ì¼ ì¤‘ ì–¼ë§ˆë‚˜ ë§Žì€ ë©”ì¼ì´ SPAMìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡ë˜ì—ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„. ì—¬ê¸°ì„œì˜ ê²°ê³¼ëŠ” 62.59%ë¡œ, ì‹¤ì œ SPAM ë©”ì¼ ì¤‘ ì•½ 62.59%ë§Œì´ SPAMìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìžˆìŒ.\n",
        "ì¦‰, SPAM ë©”ì¼ì˜ ì•½ 37.41%ëŠ” HAMìœ¼ë¡œ ìž˜ëª» ë¶„ë¥˜ë˜ì—ˆìŒ.\n",
        "\n",
        "F1 Score - 0.77 (77%):\n",
        "\n",
        "F1 ìŠ¤ì½”ì–´ëŠ” ì •ë°€ë„ì™€ ìž¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· ìœ¼ë¡œ ì •ë°€ë„ì™€ ìž¬í˜„ìœ¨ì˜ ê· í˜•ì„ ë‚˜íƒ€ë‚´ë©°, íŠ¹ížˆ ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹ì—ì„œ ìœ ìš©í•¨.\n",
        "ì—¬ê¸°ì„œëŠ” 77%ë¡œ, ë¶„ë¥˜ê¸°ì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ë©° ì¤‘ìš”í•œ ì§€í‘œ ì¤‘ í•˜ë‚˜\n",
        "\n",
        "í˜¼ë™ í–‰ë ¬(Confusion Matrix):\n",
        "array([[903, 0], [49, 82]])ëŠ” í˜¼ë™ í–‰ë ¬ì„ ë‚˜íƒ€ëƒ„.\n",
        "\n",
        "True Negative (TN): 903ê°œ - ì‹¤ì œë¡œ 'ham'(ì •ìƒ ë©”ì‹œì§€)ì´ë©°, ëª¨ë¸ë„ 'ham'ìœ¼ë¡œ ë¶„ë¥˜í•œ ê²½ìš°.\n",
        "False Positive (FP): 0ê°œ - ì‹¤ì œë¡œ 'ham'ì¸ë°, ëª¨ë¸ì´ 'spam'ìœ¼ë¡œ ìž˜ëª» ë¶„ë¥˜í•œ ê²½ìš°.\n",
        "False Negative (FN): 49ê°œ - ì‹¤ì œë¡œ 'spam'ì¸ë°, ëª¨ë¸ì´ 'ham'ìœ¼ë¡œ ìž˜ëª» ë¶„ë¥˜í•œ ê²½ìš°.\n",
        "True Positive (TP): 82ê°œ - ì‹¤ì œë¡œ 'spam'ì´ë©°, ëª¨ë¸ë„ 'spam'ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•œ ê²½ìš°.\n",
        "\n",
        "í•´ì„:\n",
        "\n",
        "1. ëª¨ë¸ì€ 'ham' ë©”ì‹œì§€ë¥¼ ë§¤ìš° ìž˜ ë¶„ë¥˜í•˜ë©°, ëª¨ë“  'ham' ë©”ì‹œì§€ë¥¼ ì˜¬ë°”ë¥´ê²Œ 'ham'ìœ¼ë¡œ ë¶„ë¥˜í•¨ (FP = 0).\n",
        "2. ê·¸ëŸ¬ë‚˜ 'spam' ë©”ì‹œì§€ ì¤‘ ì¼ë¶€ë¥¼ ìž˜ëª» ë¶„ë¥˜í•¨. 49ê°œì˜ 'spam' ë©”ì‹œì§€ë¥¼ 'ham'ìœ¼ë¡œ ìž˜ëª» ë¶„ë¥˜í•˜ì˜€ìŒ.\n",
        "3. SPAM ë©”ì¼ì„ ìž˜ ê°ì§€í•˜ëŠ” ëŠ¥ë ¥ (ìž¬í˜„ìœ¨)ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŠµë‹ˆë‹¤ (62.59%).\n",
        "ì´ëŠ” SPAM ë©”ì¼ ì¤‘ ì•½ 37%ë¥¼ ë†“ì¹œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ì´ê²ƒì€ SPAM í•„í„°ë§ì—ì„œ ì¤‘ìš”í•œ ë¬¸ì œê°€ ë  ìˆ˜ ìžˆìŒ.\n",
        "4. F1 ìŠ¤ì½”ì–´ëŠ” 77%ë¡œ ë‚˜ì˜ì§€ ì•Šì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ„.\n",
        "5. ì „ë°˜ì ìœ¼ë¡œ Confusion Matrixë¥¼ í†µí•´ ëª¨ë¸ì´ ì‹¤ì œ HAM ë©”ì¼ì— ëŒ€í•´ ë§¤ìš° ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìžˆìŒì„ í™•ì¸í•  ìˆ˜ ìžˆìŒ.\n",
        "\n"
      ],
      "metadata": {
        "id": "tvdZ0Ny7JsbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# ì¶”ê°€ ì½”ë“œ, ìŠ¤íŒ¸íŒë‹¨ ìœ ë¬´ (ìœ„ ì½”ë“œì™€ ë…ë¦½ì )\n",
        "def predict_spam(model, tokenizer, text, max_len):\n",
        "\n",
        "    # í† í°í™”\n",
        "    encoded_text = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    # íŒ¨ë”© ì²˜ë¦¬\n",
        "    padded_text = pad_sequences(encoded_text, maxlen=max_len, padding='post')\n",
        "\n",
        "    # ëª¨ë¸ ì˜ˆì¸¡\n",
        "    prediction = model.predict(padded_text)\n",
        "\n",
        "    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì˜ˆì¸¡ê°’ ì¶œë ¥\n",
        "    print(f\"Prediction Value: {prediction[0][0]:.4f}\")\n",
        "    if prediction > 0.5:\n",
        "        print(\"This is SPAM!\")\n",
        "    else:\n",
        "        print(\"This is NOT spam!\")\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì œ\n",
        "text_samples = [\n",
        "    \"Your free ringtone is waiting to be collected.\",\n",
        "    \"Click here for gain money!\",\n",
        "    \"Congratulations! You've won a $1000 gift card. Click here to claim.\",\n",
        "    \"Hi, are you available for a meeting tomorrow?\",\n",
        "    \"Your bank account has been compromised. Please send us your credentials.\",\n",
        "    \"Submission of the teaching journal in September\"\n",
        "]\n",
        "\n",
        "for text in text_samples:\n",
        "    print(f\"Testing for text: {text}\")\n",
        "    predict_spam(model, tokenizer, text, max_len)\n",
        "    print(\"-------------------------------\")\n"
      ],
      "metadata": {
        "id": "KRziaOjLFNJ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}